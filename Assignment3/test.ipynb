{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bittensorflowconda789fb1f6bc5e4117bd77a83dcb86735d",
   "display_name": "Python 3.7.6 64-bit ('tensorflow': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "9ec957caba7ae6ccc97a7fb0804bf14cbdb1f70a4904cd45a06dd27fe16a3b19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utilis as u\n",
    "import importlib\n",
    "import model as m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(u)\n",
    "filename = '/data_batch_1'\n",
    "X_train, y_train,Y_train = u.load_data(filename, reshape=False, clipping=True)\n",
    "meanX = np.mean(X_train,axis=1)\n",
    "stdX = np.std(X_train,axis=1)\n",
    "X_train = (X_train-meanX.reshape((len(meanX),1)))/stdX.reshape((len(stdX),1))\n",
    "\n",
    "filename = '/data_batch_2'\n",
    "X_val, y_val,Y_val = u.load_data(filename, reshape=False, clipping=True)\n",
    "X_val = (X_val-meanX.reshape((len(meanX),1)))/stdX.reshape((len(stdX),1))\n",
    "\n",
    "filename = '/test_batch'\n",
    "X_test, y_test,Y_test = u.load_data(filename, reshape=False, clipping=True)\n",
    "X_test = (X_test-meanX.reshape((len(meanX),1)))/stdX.reshape((len(stdX),1))\n",
    "\n",
    "data = {'X_train':X_train, 'Y_train':Y_train, 'y_train':y_train,'X_val':X_val, 'Y_val':Y_val, 'y_val':y_val, 'X_test':X_test, 'Y_test':Y_test, 'y_test':y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(m)\n",
    "mlp = m.MLP(lambda_=1e-3, dimensions=[100,60,30,10])\n",
    "P = mlp.forward_batchnorm(X_train[:100,:1])\n",
    "mlp.compute_gradients_batchnorm(X_train[:100,:1], Y_train[:,:1],P)\n",
    "mlp.compare_gradients(X_train[:100,:1], Y_train[:,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(m)\n",
    "X_train_whole, y_train_whole, Y_train_whole = u.load_data('/data_batch_1', clipping=True)\n",
    "for i in range(2,6):\n",
    "    X, y, Y = u.load_data('/data_batch_'+str(i), clipping=True)\n",
    "    X_train_whole = np.concatenate((X, X_train_whole), axis=1)\n",
    "    y_train_whole = np.concatenate((y, y_train_whole))\n",
    "    Y_train_whole = np.concatenate((Y, Y_train_whole), axis=1)\n",
    "\n",
    "X_val_small, y_val_small, Y_val_small = X_train_whole[:,-5000:], y_train_whole[-5000:], Y_train_whole[:,-5000:]\n",
    "X_train_whole, y_train_whole, Y_train_whole = X_train_whole[:,:-5000], y_train_whole[:-5000], Y_train_whole[:,:-5000]\n",
    "\n",
    "filename = '/test_batch'\n",
    "X_test_45, y_test,Y_test = u.load_data(filename, reshape=False, clipping=True)\n",
    "\n",
    "## normalize with mean and std of train set \n",
    "mean = np.mean(X_train_whole, axis=1)\n",
    "std = np.std(X_train_whole, axis=1)\n",
    "\n",
    "X_train_whole -= np.outer(mean, np.ones(X_train_whole.shape[1]))\n",
    "X_train_whole /= np.outer(std, np.ones(X_train_whole.shape[1]))\n",
    "\n",
    "X_val_small -= np.outer(mean, np.ones(X_val_small.shape[1]))\n",
    "X_val_small /= np.outer(std, np.ones(X_val_small.shape[1]))\n",
    "\n",
    "X_test_45 -= np.outer(mean, np.ones(X_test_45.shape[1]))\n",
    "X_test_45 /= np.outer(std, np.ones(X_test_45.shape[1]))\n",
    "\n",
    "\n",
    "data_45 = {'X_train':X_train_whole, 'Y_train':Y_train_whole, 'y_train':y_train_whole,'X_val':X_val_small, 'Y_val':Y_val_small, 'y_val':y_val_small, 'X_test':X_test_45, 'Y_test':Y_test,'y_test':y_test}"
   ]
  },
  {
   "source": [
    "# Train 3 layer Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [07:41<00:00, 23.08s/it]\n",
      "100%|██████████| 20/20 [07:00<00:00, 21.02s/it]\n",
      "100%|██████████| 20/20 [07:09<00:00, 21.45s/it]\n",
      "100%|██████████| 20/20 [06:49<00:00, 20.50s/it]\n",
      "100%|██████████| 20/20 [06:22<00:00, 19.15s/it]\n",
      "100%|██████████| 20/20 [06:40<00:00, 20.02s/it]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(m)\n",
    "ns = 5*45000/100\n",
    "GD_params = {\"n_batch\":100, \"eta_min\":1e-5, 'eta_max':1e-1, 'ns':ns, 'n_cycles':2, 'freq':10}\n",
    "dims = [3072,50,50,10]\n",
    "for seed in range(3):\n",
    "    mlp = m.MLP(lambda_=5e-3, dimensions=dims,seed=seed,layer_init='He')\n",
    "    model = mlp.cyclicLearning(data_45, GD_params,True, '3_layer_with_batchnorm', False, True)\n",
    "    mlp = m.MLP(lambda_=5e-3, dimensions=dims,seed=seed,layer_init='He')\n",
    "    model = mlp.cyclicLearning(data_45, GD_params,False, '3_layer_without_batchnorm', False, True)"
   ]
  },
  {
   "source": [
    "# Train 9 Layer Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [06:53<00:00, 20.67s/it]\n",
      "100%|██████████| 20/20 [06:17<00:00, 18.90s/it]\n",
      "100%|██████████| 20/20 [05:58<00:00, 17.92s/it]\n",
      "100%|██████████| 20/20 [07:37<00:00, 22.87s/it]\n",
      "100%|██████████| 20/20 [07:30<00:00, 22.55s/it]\n",
      "100%|██████████| 20/20 [07:19<00:00, 21.99s/it]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(m)\n",
    "ns = 5*45000/100\n",
    "GD_params = {\"n_batch\":100, \"eta_min\":1e-5, 'eta_max':1e-1, 'ns':ns, 'n_cycles':2, 'freq':10}\n",
    "dims = [3072,50,30,20,20,10,10,10,10]\n",
    "for seed in range(3):\n",
    "    mlp = m.MLP(lambda_=5e-3, dimensions=dims,seed=seed,layer_init='He')\n",
    "    model = mlp.cyclicLearning(data_45, GD_params,True, '9_layer_with_batchnorm', False, True)\n",
    "    mlp = m.MLP(lambda_=5e-3, dimensions=dims,seed=seed,layer_init='He')\n",
    "    model = mlp.cyclicLearning(data_45, GD_params,False, '9_layer_without_batchnorm', False, True)"
   ]
  },
  {
   "source": [
    "# Test for initialisation sensitivity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [06:18<00:00, 18.91s/it]\n",
      "100%|██████████| 20/20 [05:51<00:00, 17.57s/it]\n",
      "100%|██████████| 20/20 [06:00<00:00, 18.01s/it]\n",
      "100%|██████████| 20/20 [05:45<00:00, 17.27s/it]\n",
      "100%|██████████| 20/20 [05:56<00:00, 17.82s/it]\n",
      "100%|██████████| 20/20 [05:53<00:00, 17.68s/it]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(m)\n",
    "ns = 5*45000/100\n",
    "GD_params = {\"n_batch\":100, \"eta_min\":1e-5, 'eta_max':1e-1, 'ns':ns, 'n_cycles':2, 'freq':10}\n",
    "dims = [3072,50,50,10]\n",
    "for init in [1e-1,1e-3,1e-4]:\n",
    "    mlp = m.MLP(lambda_=5e-3, dimensions=dims,layer_init=init)\n",
    "    model = mlp.cyclicLearning(data_45, GD_params,True, f'init_{init}_with_batchnorm', False, True)\n",
    "    mlp = m.MLP(lambda_=5e-3, dimensions=dims,layer_init=init)\n",
    "    model = mlp.cyclicLearning(data_45, GD_params,False, f'init_{init}_without_batchnorm', False, True)"
   ]
  },
  {
   "source": [
    "# Search for best $\\lambda$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [08:41<00:00, 26.06s/it]\n",
      "100%|██████████| 20/20 [07:17<00:00, 21.87s/it]\n",
      "100%|██████████| 20/20 [06:59<00:00, 21.00s/it]\n",
      "100%|██████████| 20/20 [07:16<00:00, 21.82s/it]\n",
      "100%|██████████| 20/20 [07:16<00:00, 21.83s/it]\n",
      "100%|██████████| 20/20 [07:25<00:00, 22.29s/it]\n",
      "100%|██████████| 20/20 [07:02<00:00, 21.13s/it]\n",
      "100%|██████████| 20/20 [06:31<00:00, 19.60s/it]\n",
      "100%|██████████| 20/20 [07:27<00:00, 22.40s/it]\n",
      "100%|██████████| 20/20 [07:33<00:00, 22.66s/it]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(m)\n",
    "ns = 5*45000/100\n",
    "GD_params = {\"epochs\":2, \"n_batch\":100, \"eta_min\":1e-5,'eta_max':1e-1, 'ns':ns, 'n_cycles':2, 'freq':10}\n",
    "search = m.LambdaSearch(-5, -1,10)\n",
    "model = search.lambda_search(data=data_45,GDparams=GD_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5048 & 0.5094 & 0.509 & 0.511 & 0.5206 & 0.52 & 0.5234 & 0.527 & 0.512 & 0.4952 \\\n1e-05 & 3e-05 & 8e-05 & 0.00022 & 0.0006 & 0.00167 & 0.00464 & 0.01292 & 0.03594 & 0.1 \\\n"
     ]
    }
   ],
   "source": [
    "m = list(model.keys())\n",
    "print(f'{m[0]} & {m[1]} & {m[2]} & {m[3]} & {m[4]} & {m[5]} & {m[6]} & {m[7]} & {m[8]} & {m[9]} \\\\')\n",
    "print(f'{np.round(model[m[0]].lambda_,5)} & {np.round(model[m[1]].lambda_,5)} & {np.round(model[m[2]].lambda_,5)} & {np.round(model[m[3]].lambda_,5)} & {np.round(model[m[4]].lambda_,5)} & {np.round(model[m[5]].lambda_,5)} & {np.round(model[m[6]].lambda_,5)} & {np.round(model[m[7]].lambda_,5)} & {np.round(model[m[8]].lambda_,5)} & {np.round(model[m[9]].lambda_,5)} \\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [08:17<00:00, 24.85s/it]\n",
      "100%|██████████| 20/20 [07:28<00:00, 22.44s/it]\n",
      "100%|██████████| 20/20 [07:23<00:00, 22.20s/it]\n",
      "100%|██████████| 20/20 [06:46<00:00, 20.33s/it]\n",
      "100%|██████████| 20/20 [07:05<00:00, 21.30s/it]\n",
      "100%|██████████| 20/20 [07:01<00:00, 21.08s/it]\n",
      "100%|██████████| 20/20 [07:01<00:00, 21.10s/it]\n",
      "100%|██████████| 20/20 [07:30<00:00, 22.52s/it]\n",
      "100%|██████████| 20/20 [07:42<00:00, 23.12s/it]\n",
      "100%|██████████| 20/20 [06:46<00:00, 20.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import model as m \n",
    "importlib.reload(m)\n",
    "ns = 5*45000/100\n",
    "GD_params = {\"epochs\":2, \"n_batch\":100, \"eta_min\":1e-5,'eta_max':1e-1, 'ns':ns, 'n_cycles':2, 'freq':10}\n",
    "search = m.LambdaSearch(-3, -1,10)\n",
    "model = search.lambda_search(data=data_45,GDparams=GD_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5202 & 0.52 & 0.529 & 0.5234 & 0.5336 & 0.527 & 0.512 & 0.5002 & 0.4952 \\\n0.001 & 0.00167 & 0.02154 & 0.00464 & 0.00774 & 0.01292 & 0.03594 & 0.05995 & 0.1 \\\n"
     ]
    }
   ],
   "source": [
    "m = list(model.keys())\n",
    "print(f'{m[0]} & {m[1]} & {m[2]} & {m[3]} & {m[4]} & {m[5]} & {m[6]} & {m[7]} & {m[8]} \\\\')\n",
    "print(f'{np.round(model[m[0]].lambda_,5)} & {np.round(model[m[1]].lambda_,5)} & {np.round(model[m[2]].lambda_,5)} & {np.round(model[m[3]].lambda_,5)} & {np.round(model[m[4]].lambda_,5)} & {np.round(model[m[5]].lambda_,5)} & {np.round(model[m[6]].lambda_,5)} & {np.round(model[m[7]].lambda_,5)} & {np.round(model[m[8]].lambda_,5)} \\\\')"
   ]
  },
  {
   "source": [
    "# Train with the best $\\lambda$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(m)\n",
    "X_train_whole, y_train_whole, Y_train_whole = u.load_data('/data_batch_1', clipping=True)\n",
    "for i in range(2,6):\n",
    "    X, y, Y = u.load_data('/data_batch_'+str(i), clipping=True)\n",
    "    X_train_whole = np.concatenate((X, X_train_whole), axis=1)\n",
    "    y_train_whole = np.concatenate((y, y_train_whole))\n",
    "    Y_train_whole = np.concatenate((Y, Y_train_whole), axis=1)\n",
    "\n",
    "X_val_small, y_val_small, Y_val_small = X_train_whole[:,-1000:], y_train_whole[-1000:], Y_train_whole[:,-1000:]\n",
    "X_train_whole, y_train_whole, Y_train_whole = X_train_whole[:,:-1000], y_train_whole[:-1000], Y_train_whole[:,:-1000]\n",
    "\n",
    "filename = '/test_batch'\n",
    "X_test_45, y_test,Y_test = u.load_data(filename, reshape=False, clipping=True)\n",
    "\n",
    "## normalize with mean and std of train set \n",
    "mean = np.mean(X_train_whole, axis=1)\n",
    "std = np.std(X_train_whole, axis=1)\n",
    "\n",
    "X_train_whole -= np.outer(mean, np.ones(X_train_whole.shape[1]))\n",
    "X_train_whole /= np.outer(std, np.ones(X_train_whole.shape[1]))\n",
    "\n",
    "X_val_small -= np.outer(mean, np.ones(X_val_small.shape[1]))\n",
    "X_val_small /= np.outer(std, np.ones(X_val_small.shape[1]))\n",
    "\n",
    "X_test_45 -= np.outer(mean, np.ones(X_test_45.shape[1]))\n",
    "X_test_45 /= np.outer(std, np.ones(X_test_45.shape[1]))\n",
    "\n",
    "\n",
    "data_49 = {'X_train':X_train_whole, 'Y_train':Y_train_whole, 'y_train':y_train_whole,'X_val':X_val_small, 'Y_val':Y_val_small, 'y_val':y_val_small, 'X_test':X_test_45, 'Y_test':Y_test,'y_test':y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▎         | 1/40 [00:23<15:11, 23.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 2/40 [00:46<14:44, 23.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 3/40 [01:09<14:19, 23.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 4/40 [01:32<13:51, 23.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 5/40 [01:53<13:08, 22.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 6/40 [02:20<13:31, 23.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 7/40 [02:42<12:49, 23.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 8/40 [03:04<12:11, 22.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▎       | 9/40 [03:25<11:37, 22.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 10/40 [03:53<11:55, 23.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 11/40 [04:16<11:24, 23.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 12/40 [04:41<11:14, 24.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▎      | 13/40 [05:03<10:39, 23.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 14/40 [05:29<10:28, 24.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 15/40 [05:51<09:47, 23.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 16/40 [06:14<09:18, 23.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▎     | 17/40 [06:36<08:53, 23.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 18/40 [07:02<08:46, 23.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 19/40 [07:24<08:06, 23.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 20/40 [07:49<07:54, 23.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▎    | 21/40 [08:10<07:17, 23.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 22/40 [08:32<06:48, 22.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▊    | 23/40 [08:53<06:18, 22.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 24/40 [09:14<05:50, 21.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 25/40 [09:35<05:25, 21.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 26/40 [09:58<05:06, 21.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 27/40 [10:22<04:55, 22.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 28/40 [10:49<04:44, 23.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▎  | 29/40 [11:14<04:26, 24.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 30/40 [11:37<03:57, 23.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 31/40 [12:00<03:32, 23.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 32/40 [12:22<03:04, 23.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▎ | 33/40 [12:45<02:41, 23.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 34/40 [13:07<02:16, 22.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 35/40 [13:27<01:50, 22.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 36/40 [13:48<01:26, 21.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▎| 37/40 [14:08<01:03, 21.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 38/40 [14:28<00:41, 20.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 39/40 [14:49<00:20, 20.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 40/40 [15:09<00:00, 22.74s/it]\n"
     ]
    }
   ],
   "source": [
    "import model as m \n",
    "importlib.reload(m)\n",
    "ns = 5*49000/100\n",
    "GD_params = {\"n_batch\":100, \"eta_min\":1e-5, 'eta_max':1e-1, 'ns':ns, 'n_cycles':4, 'freq':10}\n",
    "dims = [3072,50,50,10]\n",
    "\n",
    "mlp = m.MLP(lambda_=0.00774, dimensions=dims,layer_init='He')\n",
    "model = mlp.cyclicLearning(data_49, GD_params,True, 'best_lambda', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}